{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241fb7e1",
   "metadata": {},
   "source": [
    "# Details about RoPE embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1829e1",
   "metadata": {},
   "source": [
    "- RoPE embedding needs to be applied to **every attention layer**. It's operating in the attention score space, not affecting the weighted_v\n",
    "- The precompute freqs_cis should have length of $2*max\\_seq\\_len$, as (m-n) could range over $2*max\\_seq\\_len$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997b1dc",
   "metadata": {},
   "source": [
    "# Implementation of RopE Positional Embedding\n",
    "\n",
    "The RoPE embedding is applied for each individual head, so after spliting head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Create precomputed RoPE embeddings\n",
    "def precompute_freqs_cis(seq_len, head_dim, base=10000.0): \n",
    "    '''Output frequency in complex space: cos + i sin\n",
    "    Output dimension is [seq_len, head_dim/2]'''\n",
    "    freqs = 1.0 / base ** (torch.arange(0, head_dim, 2).float() / head_dim)\n",
    "    t = torch.arange(seq_len)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "# 2. Apply RoPE rotation\n",
    "def apply_rope_rotation(xq, xk, freqs_cis):\n",
    "    '''\n",
    "    xq and xk has dimsion [Batch_size, Seq_len, Head_num, Head_dim],\n",
    "    freqs_cis has dimsion [Seq_len, Head_dim/2]. We need to reshape freqs_cis before broadcasting\n",
    "    '''\n",
    "\n",
    "    ## Reshape xq & xk to [B,S,D/2,2]\n",
    "    xq_ = xq.view(*xq.shape[:-1], -1, 2)\n",
    "    xk_ = xk.view(*xq.shape[:-1], -1, 2)\n",
    "\n",
    "    ## Convert to complex numbers [B,S,H,D/2]\n",
    "    xq_ = torch.view_as_complex(xq_)\n",
    "    xk_ = torch.view_as_complex(xk_)\n",
    "\n",
    "    print(xq_.shape)\n",
    "    ## Apply rotation in complex space - Complex multiplication\n",
    "    # xq_ = xq_ * freqs_cis\n",
    "\n",
    "    ## reshape freqs_cis for broadcasting\n",
    "    freqs_cis = xq_ * freqs_cis.unsqueeze(0).unsqueeze(2)\n",
    "    xq_ = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_ = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "\n",
    "    # usually there's xq_.astype(xq) and xk_.astype(xk) to make sure they sure they are on same data and device type \n",
    "    return xq_, xk_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f6073",
   "metadata": {},
   "source": [
    "# Example Usecase: RoPE + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1d51f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttentionWithRoPE(nn.Module):\n",
    "    def __init__(self, head_num, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim / head_num\n",
    "        self.Wq = nn.Linear(dim, dim)\n",
    "        self.Wk = nn.Linear(dim, dim)\n",
    "        self.Wv = nn.Linear(dim, dim)\n",
    "\n",
    "        self.precompute_freqs_cis = precompute_freqs_cis(max_seq_len*2, self.head_dim)\n",
    "\n",
    "    def forward(self, x, start_pos=0):\n",
    "        # int put x has dimension of [B, S, D]\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        # reshape to [B, S, H, Hd]\n",
    "        q = q.reshape(*q.shape[:-1], self.head_num, -1)\n",
    "        k = k.reshape(*k.shape[:-1], self.head_num, -1)\n",
    "        v = v.reshape(*v.shape[:-1], self.head_num, -1)\n",
    "\n",
    "        # apply RoPE embedding\n",
    "        q, k = apply_rope_rotation(q, k, self.precompute_freqs_cis[start_pos : start_pos+x.shape[1]])\n",
    "\n",
    "        # compute attention\n",
    "        q = q.reshape(q.shape[0], q.shape[2], q.shape[1], q.shape[3])  # reshape to [B, H, S, Hd]\n",
    "        k = k.reshape(k.shape[0], k.shape[2], k.shape[1], k.shape[3])\n",
    "        v = v.reshape(v.shape[0], v.shape[2], v.shape[1], v.shape[3])\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(2,3)) / math.sqrt(self.head_dim) # [B, H, Sq, Sk]\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)  # [B, H, Sq, Sk]\n",
    "\n",
    "        weighted_v = torch.matmul(attention_scores, v)  # [B, H, Sq, Hd]\n",
    "        weighted_v = weighted_v.reshape(weighted_v.shape[0], weighted_v.shape[2], -1)  # [B, Sq, D]\n",
    "        return weighted_v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "da631708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 8, 32])\n",
      "torch.Size([5, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B, S, D = 5, 100, 64*8\n",
    "H = 8 \n",
    "\n",
    "x = torch.randn(B, S, D).float()\n",
    "\n",
    "model = MultiHeadAttentionWithRoPE(H, D, S)\n",
    "output = model(x)\n",
    "print(output.shape)  # Expected output shape: [B, S, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e73afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
