{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeedForwardLayer():\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = 1024\n",
    "        self.W1 = np.random.randn(input_size, self.hidden_size)\n",
    "        self.b1 = np.random.randn(self.hidden_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, output_size)\n",
    "        self.b2 = np.random.randn(self.output_size)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return 1/(1+np.exp(-X))    \n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        sig_X = self.sigmoid(X)\n",
    "        return sig_X*(1-sig_X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' \n",
    "        X: input data, shape: (N, D), where N is the number of samples, D is the dimension of the input data\n",
    "        '''\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        # print((X @ self.W1).shape, self.b1.shape)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        '''\n",
    "        X: input data, shape: (N, D)\n",
    "        y: output labels, shape: (N, O)\n",
    "        '''\n",
    "\n",
    "        # MSE error\n",
    "        loss = self.a2 - y\n",
    "        z2_loss = loss * self.sigmoid_derivative(self.z2)\n",
    "        W2_grad =  self.a1.T @ z2_loss\n",
    "        b2_grad = np.sum(z2_loss, axis=0)\n",
    "        \n",
    "        a1_loss = z2_loss @ self.W2.T\n",
    "        z1_loss = a1_loss * self.sigmoid_derivative(self.z1)\n",
    "        W1_grad = X.T @ z1_loss\n",
    "        b1_grad = np.sum(z1_loss, axis=0)\n",
    "\n",
    "        # Update parameters\n",
    "        self.W2 -= learning_rate * W2_grad\n",
    "        self.b2 -= learning_rate * b2_grad\n",
    "        self.W1 -= learning_rate * W1_grad\n",
    "        self.b1 -= learning_rate * b1_grad\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output:\n",
      "[[1.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  1.  0. ]\n",
      " [0.1 0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0.2 0. ]\n",
      " [1.  1.  0.  1.  0. ]\n",
      " [1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]\n",
      " [1.  0.  0.  1.  0. ]\n",
      " [1.  1.  0.  0.  0. ]]\n",
      "True output:\n",
      "[[0.8 0.4 0.7 0.8 0.7]\n",
      " [0.  0.  0.  0.7 0.1]\n",
      " [0.3 0.  0.  0.1 0.7]\n",
      " [0.8 0.  0.  0.5 0.9]\n",
      " [1.  0.9 0.  1.  0.8]\n",
      " [0.9 0.1 0.2 0.3 1. ]\n",
      " [0.  0.  0.  0.2 0.3]\n",
      " [0.4 0.  0.3 0.1 0.4]\n",
      " [1.  0.  1.  1.  0.4]\n",
      " [1.  0.6 0.8 0.2 0.3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "\n",
    "# Generate random input data\n",
    "X = np.random.randn(num_samples, input_size)\n",
    "\n",
    "# Generate random weights for a linear transformation\n",
    "weights = np.random.randn(input_size, output_size)\n",
    "bias = np.random.randn(output_size)\n",
    "\n",
    "# Apply the linear transformation to the input data\n",
    "y = np.dot(X, weights) + bias\n",
    "\n",
    "# Apply a non-linear function (sigmoid) to the output\n",
    "y = 1 / (1 + np.exp(-y))\n",
    "\n",
    "# Add some noise to the output\n",
    "noise = np.random.normal(0, 0.1, y.shape)\n",
    "y += noise\n",
    "\n",
    "# Create an instance of the FeedForwardLayer\n",
    "layer = FeedForwardLayer(input_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "layer.train(X, y, epochs, learning_rate)\n",
    "\n",
    "# Generate test samples\n",
    "test_samples = 10\n",
    "test_X = np.random.randn(test_samples, input_size)\n",
    "\n",
    "# Generate true output values for the test samples\n",
    "test_y = np.dot(test_X, weights) + bias\n",
    "test_y = 1 / (1 + np.exp(-test_y))\n",
    "\n",
    "# Predict the output for the test samples\n",
    "predicted_y = layer.forward(test_X)\n",
    "\n",
    "print(\"Predicted output:\")\n",
    "print(np.round(predicted_y,1))\n",
    "print(\"True output:\")\n",
    "print(np.round(test_y,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward with Softmax\n",
    "\n",
    "Normally, for classification problems, we use softmax activation function in the output layer. It is usually paired with the cross-entropy loss function. Because compared to MSE loss, it has simple derivative, it is easy to implement and it is widely used in practice.  \n",
    "\n",
    "For detailed derivation of softmax and cross-entropy loss, you can refer to https://chat.openai.com/share/018aa327-76e9-43bd-a326-4c7b03f5db02.\n",
    "\n",
    "The derivative of softmax function (also for all normalizing functions) is a Jacobian matrix. It is a square matrix where the number of rows and columns is equal to the number of classes. The diagonal elements of the Jacobian matrix are the softmax function multiplied by (1 - softmax function). The off-diagonal elements are the negative of the softmax function multiplied by the softmax function. For more details https://e2eml.school/softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeedForwardWithSoftmax():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = 1024\n",
    "\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.random.randn(self.hidden_size)\n",
    "\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.random.randn(self.output_size)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def d_sigmoid(self, x):\n",
    "        return self.sigmoid(x)(1-self.sigmoid(x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    def d_softmax(self, x):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' \n",
    "        X has shape [N, I], N is number of sample. I is input dimension\n",
    "        '''\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.h1 = self.sigmoid(self.z1)\n",
    "\n",
    "        self.z2 = self.h1 @ self.W2 + self.b2\n",
    "        self.output = self.softmax(self.z2)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def cross_entropy_loss(y_true, y_pred):\n",
    "        ''' \n",
    "        y_true has shape [N, O]. N is number of sample. O is number of class.\n",
    "        '''\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "        loss = - np.sum(y_true * np.log(y_pred))\n",
    "        mean_loss = loss / y_true.shape[0]\n",
    "        return mean_loss\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        ''' \n",
    "        X has shape [N, I]\n",
    "        y has shape [N, O]\n",
    "        '''\n",
    "        # The derative of cross entropy loss and softmax\n",
    "        d_z2 = self.output - y\n",
    "        d_w2 = self.h1.T @ d_z2\n",
    "        d_b2 = np.sum(d_z2, axis=0)\n",
    "\n",
    "        d_h1 = d_z2 @ self.W2.T\n",
    "        d_z1 = d_h1 * self.d_sigmoid(self.z1)\n",
    "        d_W1 = X.T @ d_z1\n",
    "        d_b1 = np.sum(d_z1, axis=0)\n",
    "\n",
    "        self.W1 -= learning_rate*d_W1\n",
    "        self.b1 -= learning_rate*d_b1\n",
    "        self.W2 -= learning_rate*d_w2\n",
    "        self.b2 -= learning_rate*d_b2\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Jocabian matrix of softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16285523, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.07200342, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.08273631, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.04645867, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.06146745,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.15080146, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04559796, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11018178, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04061396, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.22728377]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_jacob(x):\n",
    "    ''' \n",
    "    Jacobian matrix for softmax is a symmetric matrix\n",
    "    '''\n",
    "    return np.diag(softmax(x)) - np.outer(softmax(x),softmax(x))\n",
    "\n",
    "\n",
    "x = np.random.randn(10) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
